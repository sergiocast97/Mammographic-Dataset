\documentclass[a4paper,12pt]{article}

% Packages
\usepackage{booktabs}
\usepackage{graphicx, verbatim}
\usepackage{amsmath}
\usepackage{amssymb}
\usepackage{amscd}
\usepackage{lipsum}
\usepackage{todonotes}
\usepackage[tableposition=top]{caption}
\usepackage{ifthen}
\usepackage[utf8]{inputenc}
\usepackage{graphicx}
\usepackage{caption}
\usepackage{subcaption}
\setlength{\textwidth}{6.5in} 
\setlength{\textheight}{9in}
\setlength{\oddsidemargin}{0in} 
\setlength{\evensidemargin}{0in}
\setlength{\topmargin}{-1.5cm}
\setlength{\parindent}{0cm}
\usepackage{setspace}
\usepackage{float}
\usepackage{amssymb}
\usepackage[utf8]{inputenc}
\usepackage{fancyhdr}
\usepackage{tabularx}
\usepackage{hyperref}
\hypersetup{
  colorlinks   = true, % Colours links instead of ugly boxes
  urlcolor     = blue, % Colour for external hyperlinks
  linkcolor    = blue, % Colour of internal links
  citecolor    = red   % Colour of citations
}

% Document Styling
\rfoot{Sergio Castillo \thepage}
\singlespacing
\usepackage[affil-it]{authblk} 
\usepackage{etoolbox}
\usepackage{lmodern}

% Citation Package
\usepackage[backend=bibtex ,sorting=none]{biblatex}
\bibliography{references}
\begin{filecontents*}{references.bib}
\end{filecontents*}

% Metadata
\title{\textbf{\Huge Mammographic Mass Data Set}}
\author{\Large Sergio Castillo, 1513228@rgu.ac.uk}
\date{22nd November 2019}

% Document Start
\begin{document}
%\SweaveOpts{concordance=TRUE}

% Cover Page
\input{titlepage}

% Table of Contents
\pagenumbering{roman}
\tableofcontents
\newpage
\pagenumbering{arabic}

% Overview
\section{Overview}\label{overview}
The purpose of this report is to analyze a given dataset, in order to reach certain conclusions and be able to predict an outcome with the highest accuracy possible, thus showing competence in understanding, identifying, applying and evaluating machine learning algorithms. In order to do that, I will be using a data set focused on the prediction of malicious mammographic masses based on the result of BI-RAD (Breast Imaging-Reporting and Data System) assessments.

% Data Exploration
\section{Data Exploration}\label{data_exploration}

% The Dataset
\subsection{The Dataset}\label{dataset}
I've chosen this dataset because I am interested in learning how data analysis and machine learning can be used in order to save people's lives. Additionally, the number of attributes and instances seemed adequate for analysis.

\subsubsection{Dataset Name}\label{name}
Mammographic Mass

\subsubsection{Dataset Source}\label{source}
The dataset was obtained from the UCI Machine Learning Repository\cite{datasetOrigin}.
The origin of the dataset is a paper published by M.Elter, R.Schulz-Wendtland and T.Wittenberg, titled \textbf{"The prediction of breast cancer biopsy outcomes using two CAD approaches that both emphasize an intelligible decision process"}\cite{researchPaper}. 

\subsubsection{Dataset Acquirement Date}\label{date}
The dataset was obtained on the 4th of October 2019\\
The original paper was published in 2007, and the dataset was donated on the 29th of October, 2007.

\clearpage

% Problem Statement
\subsection{Problem Statement \& Data Exploration}\label{dataexp}
This dataset contains information of 961 patients' BI-RAD assessment, having different parameters of the BI-RAD attributes, patient's age and the result of such assessment
Mammography is the most effective method for breast cancer screening available today. However, the low positive predictive value of breast biopsy resulting from mammogram interpretation leads to approximately 70\% unnecessary biopsies with benign outcomes. To reduce the high number of unnecessary breast biopsies, several computer-aided diagnosis (CAD) systems have been proposed in the last years.These systems help physicians in their decision to perform a breast biopsy on a suspicious lesion seen in a mammogram or to perform a short term follow-up examination instead.

\subsubsection{Objective}\label{objective}
This data set can be used to predict the severity (benign or malignant) of a mammographic mass lesion from BI-RADS ( Breast Imaging-Reporting and Data System) attributes and the patient's age.

<<echo=FALSE, eval=TRUE>>=

# Set directory to current
#setwd("C:/Users/sergi/Google Drive/Uni/Y-03/CM3111 - Big Data Analytics/Coursework/Report")

# Importing the data
df <- read.table('data/mammographic_masses.data', header=FALSE, sep=",", na.strings="?")

# Changing the column names
names(df) <- c("Assessment","Age","Shape","Margin","Density", "Severity")

# Converting into categorical
df$Assessment <- factor(df$Assessment)
df$Shape      <- factor(df$Shape)
df$Margin     <- factor(df$Margin)
df$Density    <- factor(df$Density, order=TRUE, levels=c("1","2","3","4"))
df$Severity   <- factor(df$Severity)

@

<<echo=TRUE, eval=TRUE>>=

# Dataset dimensions
nrow(df)
ncol(df)
dim(df)

# Show some rows
df[1:10,]

@

\clearpage

\subsubsection{Attributes}\label{attributes}
The attributes provide information about the patients' age, mammograpic mass results and the severity of the mass. There are 6 attributes in total, from which 4 are predictive attributes, 1 is non-predictive and 1 is the goal field. 

\begin{enumerate}

\item \textbf{Assessment}: Results of the BI-RADS, an indication of how well a CAD system performs compared to the radiologists
\begin{itemize}
  \item 0: Definitely benign - 6: Highly suggestive of malignacy
  \item Ordinal Categorical Variable
  \item Non-Predictive Attribute
\end{itemize}

\item \textbf{Age}: Patient's age in years
\begin{itemize}
  \item Integer Variable
  \item Predictive Attribute
\end{itemize}

\item \textbf{Shape}: Mass shape
\begin{itemize}
  \item 1: Round, 2: Oval, 3: Lobular, 4: Irregular
  \item Nominal Categorical Variable
  \item Predictive Attribute
\end{itemize}

\item \textbf{Margin}: Mass margin
\begin{itemize}
  \item 1: Circumscribed, 2: Microlobulated, 3: Obscured, 4: Ill-defined, 5: Spiculated
  \item Nominal Categorical Variable
  \item Predictive Attribute
\end{itemize}

\item \textbf{Density}: Mass density
\begin{itemize}
  \item 1: High, 2: Iso, 3: Low, 4: Fat-containing
  \item Ordinal Categorical Variable
  \item Predictive Attribute
\end{itemize}

\item \textbf{Severity}: Severity of the mammographic mass
\begin{itemize}
  \item 0: Bening, Malignant: 1
  \item Binomial Categorical Variable
  \item Goal Attribute
\end{itemize}
\end{enumerate}

<<echo=TRUE, eval=TRUE>>=
# Show Column names
names(df)
@

\clearpage

\subsubsection{Class Distribution}\label{class_distribution}
The goal field on the dataset is the severity of the mammographic mass, which can either be benign (i.e Non-Harmful) or malicious.
<<echo=TRUE, eval=TRUE>>=
# Finding how many unique fields there are, and the unique values
length(unique(df$Severity))
unique(df$Severity)
@
As shown in the bar plot below, the dataset contains 961 entries, for 516 benign and 445 malignant masses.\\

\begin{figure}[H]
{\centering
<<echo=TRUE, eval=TRUE, out.width=".5\\linewidth">>=

# Class distribution
severityFreq <- table(df$Severity)
barplot( 
  severityFreq,
  col = c('steelblue4','coral1'),
  main="Severity Frequency",
  names.arg=c("Benign","Malicious"),
  ylab="Number of patients"
)


@
\caption {Class distribution of the severity}
\label{fig1}
}
\end {figure}

\clearpage

\subsubsection{Dataset Summary}\label{summary}

The summary displays a briefing of every column, showing statistical iformation about the data in every entry. The summary can either be numerical (Which shows information such as the maximum and minimum values, median and quartiles) or categorical, which shows the frequency of each one of the categorical values.\\
In this dataset, only the Patient's age is numerical. The rest of the attributes only show the frequency.\\

% Age
{\large Age Summary}\\
The patient Age attribute is a numerical attribute, so the summary shows the minimum, first quartile, median, mean, third quartile and maximum values of the data
<<echo=TRUE, eval=FALSE>>=
# Age    Summary
summary(df$Age)
@
\begin{table}[H]
\centering
\begin{tabular}{ c c c c c c c }
Min   & 1st Qu  & Median  & Mean  & 3rd Qu  & Max   & Missing \\
\hline
18.00 & 45.00   & 57.00   & 55.49 & 66.00   & 96.00 & 5
\end{tabular}
\end{table}

\begin{figure}[H]
{\centering
<<echo=TRUE, eval=TRUE, out.width=".5\\linewidth">>=
Age <- df$Age
hist(
  Age,
  xlab="Patient Age",
  col="turquoise3",
)
@
\caption {Histogram of the Patients' Age}
\label{fig2}
}
\end {figure}

% Shape
{\large Shape Summary}\\
The mass Shape attribute is a categorical attribute, so the summary shows the value frequency
<<echo=TRUE, eval=FALSE>>=
# Shape Summary
summary(df$Shape)
@
\begin{table}[H]
\centering
\begin{tabular}{ c c c c c }
Round & Oval & Lobular & Irregular & Missing \\
\hline
224   & 211  & 95      & 400       & 31
\end{tabular}
\end{table}

% Margin
{\large Margin Summary}\\
The mass Margin attribute is a categorical attribute, so the summary shows the value frequency
<<echo=TRUE, eval=FALSE>>=
# Margin Summary
summary(df$Margin)
@
\begin{table}[H]
\centering
\begin{tabular}{ c c c c c c }
Circumscribed & Microlobulated & Obscured & Ill-defined & Spiculated & Missing \\
\hline
357           & 24             & 116      & 280         & 136        & 48  
\end{tabular}
\end{table}

% Density
{\large Density Summary}\\
The mass Density attribute is a categorical attribute, so the summary shows the value frequency
<<echo=TRUE, eval=FALSE>>=
# Density Summary
summary(df$Density)
@
\begin{table}[H]
\centering
\begin{tabular}{ c c c c c }
High  & Iso  & Low  & Fat-Containing & Missing \\
\hline
16    & 59   & 798  & 12             & 76
\end{tabular}
\end{table}

\clearpage

\subsubsection{Attribute Correlation}\label{correlation}
Correlation measures the relationship between two measurements. This dataset only contains one numerical attribute, so there's no chance to create a numerical correlation between the different values. However, a relationship can be established between the age of the patient and the severity of the mammographic mass.This relation is known, but the other attributes also contribute to it.

\begin{figure}[H]
{\centering
<<echo=TRUE, eval=TRUE, out.width=".75\\linewidth">>=
# Finding how attributes compare to goal
plot(
  data = df,
  Age~Severity,
  col = c('steelblue4','coral1'),
  names.arg=c("Benign","Malicious")
  
)
@
\caption {Correlation between the Age and the severity of the Mammographic mass}
\label{fig3}
}
\end {figure}

\clearpage

In the following page, correlations 3 Age and different Mammographic attributes can be found [~\ref{fig4}], [~\ref{fig5}], [~\ref{fig6}]. As mentioned, there is a relation between the age and the type of mass, but it's not enough to reach a conclusion.

\begin{figure}[H]
{\centering
<<echo=TRUE, eval=TRUE, out.width=".5\\linewidth">>=
library(datasets)
data <- transform(df, Shape)
boxplot(Age~Shape, data, xlab="Shape", ylab="Age")
@
\caption {Relation between Age and Mass Shape (Round, Oval, Lobular, Irregular)}
\label{fig4}
}
\end {figure}

\begin{figure}[H]
{\centering
<<echo=TRUE, eval=TRUE, out.width=".5\\linewidth">>=
data <- transform(df, Margin)
boxplot(Age~Margin, data,xlab="Margin",ylab="Age")
@
\caption {Relation between Age and Mass Margin (Circumscribed, Microlobulated, Obscured, Ill-defined, Spiculated)}
\label{fig5}
}
\end {figure}

\begin{figure}[H]
{\centering
<<echo=TRUE, eval=TRUE, out.width=".35\\linewidth">>=
data <- transform(df, Density)
boxplot(Age~Density, data,xlab="Density",ylab="Age")
@
\caption {Relation between Age and the Mass Density (High, Iso, Low, Fat-containing)}
\label{fig6}
}
\end {figure}
  
\clearpage

% Data Pre-processing
\section{Data Pre-processing}\label{preprocess}
In order to analyze the data, the attributes must first be adequated for study. This is known as pre-processing

% Irrelevant Columns
\subsection{Irrelevant Columns}\label{irrelevant_columns}
Before going any further, it has been previously mentioned that the Assessment column is a non-predictive attribute, since it measures the effectiveness of CAD Systems. Therefore, the column is not relevant for data mining, and it should be dropped.

<<echo=TRUE, eval=TRUE>>=
# Backing up the data frame
df_full <- df

# Dropping the irrelevant column
df$Assessment <- NULL

# Showing the data frame columns without the dropped one
names(df)
@

% Missing Values
\subsection{Missing Values}\label{missing_values}
One of the main issues with the dataset is the number of missing values. When taking a look at the raw data, it can be appreciated that the way missing values were handled was by writing a question mark "?" in place of the value.\\
The data frame will need to have those missing values recognized, so when parsing the file as a data frame, missing values must be specified

<<echo=TRUE, eval=TRUE>>=
# Importing the data
dataFrame <- read.table('data/mammographic_masses.data',
                        header=FALSE, sep=",", na.strings="?")

# Showing some rows
df[1:5,]
@
\clearpage
Now that missing values have been identified, it is necessary to know which values are missing:

<<echo=TRUE, eval=TRUE>>=
sum(is.na(df))
sum(is.na(df$Age))
sum(is.na(df$Shape))
sum(is.na(df$Margin))
sum(is.na(df$Density))
sum(is.na(df$Severity))
@

\begin{table}[H]
\begin{tabular}{ c c c c c c }
Age & Shape & Margin & Density & Severity & Total \\
\hline
5   & 31    & 48     & 76      & 48       & 162  
\end{tabular}
\end{table}

\clearpage

\subsubsection{Filling missing values}\label{filling_gaps}
With numerical values, a way to deal with missing values is by replacing the blank attribute with the mean of such attribute.
Since the age is the only numerical attribute, we can only use this method here.
<<echo=TRUE, eval=TRUE>>=

# Defining the function
fillMissings <- function(x) {
  replace(
    x,
    is.na(x),
    mean(x[!is.na(x)]))
}

# Applying the function to the Age column
df$Age <- fillMissings(df$Age)

# Verifying the number of missing values on the age column
sum(is.na(df$Age))

@

\subsubsection{Deleting rows}\label{deleting_rows}
Since we cannot fill the missing values, due to the categorical nature of the attributes, we have no other option but to drop the rows that contain them.

<<echo=TRUE, eval=TRUE>>=

# Deleting rows with misssing factor attributes
df <- df[!is.na(df$Shape),]
df <- df[!is.na(df$Margin),]
df <- df[!is.na(df$Density),]

@

We can now observe that there are no missing values in our data frame

<<echo=TRUE, eval=TRUE>>=
sum(is.na(df))
@

\clearpage

% Missing Header
\subsection{Missing Header}\label{missing_header}
Another issue with the dataset is the lack of headers. We may know the meaning of every column after reading about the dataset, but we still need a way to identify each one of the columns.

<<echo=TRUE, eval=TRUE>>=
# Showing the data frame header
names(dataFrame)

# Changing the column names
names(dataFrame) <- c("Assessment","Age","Shape","Margin","Density", "Severity")

# Showing the column names
names(dataFrame)
@

\subsection{Categorical Values}\label{categorical_values}
As previously mentioned, most of the values are categorical, rather than numerical. This means that it can't just be measured as a number, but as propertie of the entries. After dropping unnecessary columns, the categorical attributes are the following:

\begin{enumerate}
\item \textbf{Shape}: Shape of the mammographic mass. (Nominal)
\item \textbf{Margin}: Margin of the mammographic mass. (Nominal)
\item \textbf{Density}: Density of the mammographic mass. (Ordinal)
\item \textbf{Severity}: Severity of the mammographic mass. (Binominal)
\end{enumerate}

Fortunately enough, the categorical attributes already come as numerical values. If this was not the case and they were represented as, for instance, strings, such strings would need to be converted into numerical values.\\
Now, the attributes will be converted into categorical values.

<<echo=TRUE, eval=TRUE>>=

# Turning the attributes into categofical
df$Shape      <- factor(df$Shape)
df$Margin     <- factor(df$Margin)
df$Density    <- factor(df$Density, order=TRUE, levels=c("1","2","3","4"))
df$Severity   <- factor(df$Severity)

# Checking whether the fields are factors
is.factor(df$Age)
is.factor(df$Shape)
is.factor(df$Margin)
is.factor(df$Density)
is.factor(df$Severity)

@

\subsection{Data Normalization}\label{data_normalization}
Since there are no noticeable irregularities in the dataset, such as outliers or skewed values, there is no need to normalize the dataset.

\clearpage

% Modelling Section

\section{Modelling}\label{modelling}
Now that the dataset has been explored and there is a better idea of what needs to be predicted, we can start building and testing models.

\subsection{Creating Subsets}\label{subsets}
Before creating predictive models, the dataset will be split into a training and a testing set, in order to avoid future problems with overfitting or underfitting.

<<echo=TRUE, eval=TRUE, warning=FALSE, message=FALSE>>=
# Downloading necessary libraries
library(ISLR)
library(dplyr)
library(ROCR)
@

<<echo=TRUE, eval=TRUE>>=

# Getting the training dataset
train<-sample_frac( df , 0.7)
sid<-as.numeric(rownames(train))
nrow(train)

# Getting the test dataset
validation <- df[-sid,]
nrow(validation)

if ( nrow(train) + nrow(validation) == nrow(df) ){
  print("The dataset has been split properly")
} else {
  print("The split of the dataset was not correct")
}

@

\clearpage

\subsection{Implementing a Model}\label{model}
Now that the dataset has been split, the training subset can be used to start designing the model. For this case, Logistic Regression will be used. Logistic regression is used to model the probability that an ‘event’ will occur. In other words, the dependent variable, now p(X), assumes a value between 0 and 1. This is suitable for the current dataset, since what we are trying to predict is whether a mammographic mass can be benign of malicious (0 or 1).

<<echo=TRUE, eval=TRUE>>=

# Implementing the Logistic Regression model

regressionModel <- glm(
  Severity ~ Age+Shape+Margin+Density, # Severity against parameters
  data=train,                          # Using training data
  family=binomial                      # Goal is a binomial value
)

# Spitting the summary of the model
summary(regressionModel)

@

\subsection{Model Evaluation}\label{evaluation}
After applying certain model to a dataset it is necessary to evaluate its performance. In order to dot that, a confusion matrix will be used, in which the prediction column that was produced during the model needs to be tested against the actual class values that already existed on the dataset. This will produce the following possible results:
\begin{itemize}
  \item \textbf{True Positive (TP)}: correctly classified as the class of interest
  \item \textbf{True Negative(TN)}: Correctly classified as not the class of interest
  \item \textbf{False Positive(FP)}: Incorrectly classified as the class of interest
  \item \textbf{False Negative(FN)}: Incorrectly classified as not the class of interest
\end{itemize}

<<echo=TRUE, eval=TRUE>>=

# Using a Confusion Matrix to verify the used model

# Creating a vector that will store the predictions
predictedValues <- as.integer(regressionModel$fitted.values > 0.5)

# Testing predictions against the severity
error <- mean(predictedValues != train$Severity)

# Outputting the overall accuracy of the model
( training_accuracy <- (1 - error) * 100)

@

The model has been tested using the training subset. However, in order to perform a thorough analysis of the performance, it is advisable to predict the severity of the mammographic masses against the validation subset. In order to do that, a prediction is created by using the model and the testing dataset, with which the accuracy can be calculated.

<<echo=TRUE, eval=TRUE>>=

# Generating a prediction
probabilities <- predict(
  regressionModel,                              # Used model
  newdata=subset(validation,select=c(1,2,3,4)), # Test dataset
  type="response"                               # Type of prediction
)

# Store the predictions as 1 or 0
predictions <- ifelse(probabilities>0.5, 1, 0)

# Output the accuracy
( test_accuracy <- mean(predictions==validation$Severity) )

@

Finally, a ROC curve can be used to measure the performance of a model:
\begin{itemize}
  \item The true positive rate is the sensitivity: the fraction of defaulters that are correctly identified, using a given threshold value.
  \item The false positive rate is 1-specificity: the fraction of non-defaulters that we classify incorrectly as defaulters, using that same threshold value.
\end{itemize}

\begin{figure}[H]
{\centering
<<echo=TRUE, eval=TRUE, out.width=".5\\linewidth">>=
# ROC Model
pr <- prediction(probabilities, validation$Severity)
prf <- performance( pr, measure = "tpr", x.measure = "fpr")
plot(prf,frame=FALSE)
@
\caption {ROC curve of the regression model}
\label{fig7}
}
\end {figure}

\clearpage

\section{Improving Performance}\label{improving_performance}
Even though the model has achieved a fairly decent performance, it can definitely be improved. Small tweaks in the dataset, model and validation might have a positive impact on the accuracy, which would lead to a more trustworthy prediction of a potential malignant lesion, hence saving lives. I've chosen the following methods to improve my dataset's model:
\begin{itemize}
  \item \textbf{Filling missing values}: By dealing with missing values in a better way, the dataset size may increase, which leads to more accurate models.
  \item \textbf{Cross Validation}: When using training and testing datasets, there must be a tradeoff between maximizing training and validation dataset size, which can lead to inaccurate models. By using different validation methods, the performance of a given model may increase.
  \item \textbf{Different Models}: Using different modelling algorithms may vary the prediction accuracy for any given dataset, so it is a good idea to use more complex models.
\end{itemize}

<<echo=TRUE, eval=TRUE, warning=FALSE, message=FALSE>>=
# Downloading necessary libraries
library(caret)
library(mice)
@

\subsection{Filling Missing Values}\label{filling_missing}
Having a large enough dataset is a very relevant factor when it comes to perfecting a machine learning algorithm. The bigger the training data, the better the adjustments, and the more accurate the system may become. On any given dataset, there may be missing values, and while sometimes there is no way to recreate such missing values and entire rows must be dropped, an effort must be made in order to save as much information as possible.\\
Earlier in this coursework, it was assumed that all missing categorical values were supposed to be deleted, without trying to handle them. However, this was a mistake, and there are sevetral algorithms that deal with missing categorical values \cite{mekala_2019}. In this case, the package of choice has been MICE (Multivariate Imputation via Chained Equations), one of the commonly used package by R users. Creating multiple imputations as compared to a single imputation takes care of uncertainty in missing values. MICE assumes that the missing data are Missing at Random (MAR), which means that the probability that a value is missing depends only on observed value and can be predicted using them. It imputes data on a variable by variable basis by specifying an imputation model per variable.

<<echo=TRUE, eval=TRUE>>=

# Getting our new dataframe ready
new_df <- df_full
new_df$Assessment <- NULL
new_df$Age <- fillMissings(new_df$Age)

# Using MICE to fill missing categorical values
new_df <- mice(
  new_df,           # Using the newDF dataframe
  m=1,              # Generating only 1 dataframe
  maxit=10,         # Number of Iterations
  method='polyreg', # Method of imputation (Polyreg is used for categorical values)
  seed=500          # Seed number
)

# Choosing the generated dataframe
new_df <- complete(new_df,1)

# Checking the number of missing values
sum(is.na(new_df))

@

\subsection{Cross Validation}\label{cross_validation}
As mentioned earlier, dividing datasets into training and a validation subsets is an easy way to train a model, but it can lead to tradeoffs during the different stages of the model creation. There are many ways to validate models, but in this case the best approach is to use Cross Validation method. The k-fold cross validation method involves splitting the dataset into k-subsets. During the training phase each subset is held out while the model is trained on all other subsets. This process is completed until accuracy is determine for each instance in the dataset, and an overall accuracy estimate is provided.

<<echo=TRUE, eval=TRUE>>=

# Create a data partition for Cross Validation
inTrain <- createDataPartition(
  y=new_df$Severity, # Have Severity as the classifying value
  p=.7,              # Assign 70% of the data to training
  list=FALSE)

# Generating the training subset
new_training <- new_df[inTrain,]
nrow(new_training)

# Generating the validating subset
new_validating <- new_df [-inTrain,]
nrow(new_validating)

if ( nrow(new_training) + nrow(new_validating) == nrow(new_df) ){
  print("The dataset has been split properly")
} else {
  print("The split of the dataset was not correct")
}

@


\subsection{Different Models}\label{different_models}
On this particular dataset, having an accurate prediction can drastically influence a patient, since a false negative means that a  mammographic mass could develop into a potentially untreated cancerous condition.\\
The previously used logistic regression algorithm may have given decent results, but it still might be a good idea to test a different model. In this case, the chosen predictive model will be "Random Forest", which consists of a large number of individual decision trees that operate as an ensemble. Each individual tree in the random forest spits out a class prediction and the class with the most votes becomes our model’s prediction \cite{yiu_2019}.

<<echo=TRUE, eval=TRUE>>=

# Using the Random Forest Algorithm
#fitModel <- train(Species~.,data=training,method="rf",prox=TRUE)

@

\subsection{Conclusion}\label{improvement_conclusion}
Now that we have imrpoved different parts of our dataset and model, it is time to test its performance. Just like during the modelling stage, we will test the model against the validation subset, and verify the model accuracy.

<<echo=TRUE, eval=TRUE>>=

# Testing the improved model accuracy

@

\clearpage

\section{References}\label{pubs}

\printbibliography[heading =none]


\clearpage


\end{document}
